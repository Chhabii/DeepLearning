{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Layers and Blocks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFRdqDEAJJTPEsSji2xvk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RxnAch/DeepLearning/blob/main/Layers_and_Blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1vIYlMHsE6e"
      },
      "source": [
        "#Layers and Blocks\n",
        "\n",
        "To implement complex network,we introduce the concept of neural network,***block***.\n",
        "\n",
        "A Block could describe a single layer, a component consisting of multiple layers, or the entire model itself.\n",
        "\n",
        "From a programming standpoint, a block is represented by a class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY4TkEjnto93"
      },
      "source": [
        "The following code generates a network with one fully-connected hidden layer with 256 units and ReLU activation, followed by a fully-connected output layer with 10 units(no activation function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWZfOnW1tBze",
        "outputId": "233e8295-5461-4936-db62-59a0c6396e4e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "net = nn.Sequential(nn.Linear(20,256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256,10))\n",
        "X = torch.rand(2,20)\n",
        "net(X)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1381, -0.1505, -0.2038, -0.0556, -0.0282, -0.0117,  0.0874, -0.1312,\n",
              "         -0.0394, -0.0584],\n",
              "        [ 0.0798, -0.1943, -0.1880,  0.0112, -0.0879,  0.0390,  0.1357, -0.0288,\n",
              "         -0.0435, -0.0846]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMpWycT_wMu-"
      },
      "source": [
        "#A custom Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3y-xap4wRQt"
      },
      "source": [
        "Our block must possess:\n",
        "\n",
        "1. Ingest input data as arguments to its forward propagation function.\n",
        "\n",
        "2. Generate an output by having the forward propagation function return a value. Note that the output may have a different shape from the input. For example, the first fully-connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
        "\n",
        "3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation function. Typically this happens automatically.\n",
        "\n",
        "4. Store and provide access to those parameters necessary to execute the forward propagation computation.\n",
        "\n",
        "5. Initialize model parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF3fziSfxSaW"
      },
      "source": [
        "class MLP(nn.Module): \n",
        "  #Declare a layer with model parameters.Here, we declare two fully connected layers\n",
        "  def __init__(self):\n",
        "    # Call the constructor of the `MLP` parent class `Module` to perform\n",
        "    # the necessary initialization.\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(20,256) #hidden layer\n",
        "    self.out = nn.Linear(256,10) #output layer\n",
        "  #Define forward propagation with input \"X\"\n",
        "  def forward(self,X):\n",
        "    return self.out(F.relu(self.hidden(X)))\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNT8pbu--fFp",
        "outputId": "86461554-f443-4c21-d25e-204474a655f3"
      },
      "source": [
        "net = MLP()\n",
        "net(X)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0944,  0.0666,  0.0340,  0.0122,  0.0773, -0.1533, -0.0289,  0.1357,\n",
              "          0.0163, -0.2042],\n",
              "        [-0.1267, -0.0008,  0.0189,  0.1058, -0.0131, -0.0216,  0.0414,  0.0085,\n",
              "          0.1683, -0.3031]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1YPuz86xfDd"
      },
      "source": [
        "#My Sequential Block\n",
        "\n",
        "To build our own simplified **MySequential**,\n",
        "we just need to define two key functions:\n",
        "\n",
        "1. A function to append blocks(module) one by one to a list.\n",
        "2. Forward Propagation function in a order as given in list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Btlppq_0sY"
      },
      "source": [
        "class MySequential(nn.Module):\n",
        "  def __init__(self,*args):\n",
        "    super().__init__()\n",
        "    for idx,module in enumerate(args): #\n",
        "      self._module[str(idx)] = module   #self.blocklist = [block for block in args]\n",
        "  def forward(self,X):\n",
        "    for block in self._modules.values():\n",
        "      X = block(X)\n",
        "    return X\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzw0kvnZy1wW"
      },
      "source": [
        "In the __init__ method, we add every module to the ordered dictionary _modules one by one.\n",
        "\n",
        "When our MySequential’s forward propagation function is invoked, each added block is executed in the order in which they were added. We can now reimplement an MLP using our MySequential class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL-9XpZ0Ky71",
        "outputId": "c6eadb0a-ced7-4a51-83a0-976aafac202a"
      },
      "source": [
        "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
        "net(X)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0383, -0.1540, -0.1938, -0.1322, -0.1173,  0.1900, -0.0340,  0.1142,\n",
              "          0.1248, -0.0362],\n",
              "        [ 0.0403, -0.1120, -0.1187, -0.2979, -0.0204,  0.0447, -0.0974,  0.1629,\n",
              "          0.0900,  0.1336]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pk-E4NyzXFX"
      },
      "source": [
        "#Executing code in the forward propagation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hKyoe3qzjxs"
      },
      "source": [
        "When greater flexibility is required, we will want to define our own blocks. For example, we might want to execute Python’s control flow within the forward propagation function. Moreover, we might want to perform arbitrary mathematical operations, not simply relying on predefined neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn_F8rRaLwbc"
      },
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #random weight parameters that will not compute gradients and thereafter remains constant\n",
        "    self.rand_weight = torch.rand((20,20),requires_grad = False)\n",
        "    self.linear = nn.Linear(20,20)\n",
        "#Foraward Propagation function\n",
        "#We can perform any mathematical operations. for now f(x,w) = c.w^T.x , c is constant which is not updated.\n",
        "  def forward(self,X):\n",
        "    X = self.linear(X)\n",
        "    X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
        "    X = self.linear(X)\n",
        "    #Control Flow\n",
        "    while X.abs().sum()>1:\n",
        "      X/=2\n",
        "    return X.sum()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1O2NDhr0wO0"
      },
      "source": [
        "Note that this particular operation may not be useful in any real-world task. Our point is only to show you how to integrate arbitrary code into the flow of your neural network computations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSp7vIgOSVXG",
        "outputId": "d4d73340-beed-4e91-93c5-f0a5f01269c6"
      },
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1784, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT9Ugs8c01Qd"
      },
      "source": [
        "We can mix and match above various ways of assembling blocks together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3BQ6_cUQjNY",
        "outputId": "8e8427e7-d697-4a6f-89c7-dcd728c81cb0"
      },
      "source": [
        "class NestMLP(nn.Module):#nest blocks\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),nn.Linear(64,32),nn.ReLU())#a block\n",
        "    self.linear = nn.Linear(32,16) # a block\n",
        "    #define forward propagation\n",
        "  def forward(self,X):\n",
        "    return self.linear(self.net(X))\n",
        "chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP()) #Sequential module chain blocks together.\n",
        "chimera(X)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.3495, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}